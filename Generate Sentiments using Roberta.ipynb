{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import contractions\n",
    "import os\n",
    "import text_preprocessing as tp\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ Change the filename assigned to you\n",
    "\n",
    "1. Get the assigned file from https://mylambton.sharepoint.com/:f:/r/sites/NLPandSocialMediaAnalytics/Shared%20Documents/General/Split%20Dataset%20(Movie%20Reviews)?csf=1&web=1&e=1EaRaL\n",
    "2. Create 'split_dataset' inside the 'dataset' folder. Put all the files inside 'split_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'reviews'\n",
    "source_review_file_diretory_path = \"./dataset/\"\n",
    "split_dataset_directory_path = './dataset/split_dataset/'    # contains the split files\n",
    "done_directory_path = './dataset/split_dataset/done/' # contains split files that were processed\n",
    "sentiment_directory_path = './dataset/split_dataset/sentiment/' # contains split files that were processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NO NEED TO EXECUTE THE CODE SECTION BELOW. IT IS USED TO DIVIDE RECORDS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_and_save_csv(input_file, output_prefix, chunk_size):\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(input_file)\n",
    "\n",
    "#     # Split the DataFrame into chunks\n",
    "#     chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "#     # Save each chunk to a separate CSV file\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         output_file = f\"{output_prefix}_{i + 1}.csv\"\n",
    "#         chunk.to_csv(output_file, index=False)\n",
    "#         print(f\"Saved {len(chunk)} records to {output_file}\")\n",
    "\n",
    "\n",
    "# if not os.path.exists(split_dataset_directory_path):\n",
    "#     os.makedirs(split_dataset_directory_path)\n",
    "\n",
    "# input_csv = source_review_file_diretory_path + filename + \".csv\"  # Replace with your actual CSV file\n",
    "# output_prefix = split_dataset_directory_path + filename      # Prefix for output files\n",
    "# chunk_size = 100                     # Number of records per chunk\n",
    "\n",
    "# split_and_save_csv(input_csv, output_prefix, chunk_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling records with sentiment using Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading source the file: ./dataset/split_dataset/reviews_4201.csv\n",
      "Parallel processing of movie reviews...\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "# Initialize the English stop words list\n",
    "list_of_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def data_preprocessing(text):\n",
    "    text = tp.remove_email_address(text)\n",
    "    text = tp.remove_hyperlink(text)\n",
    "    text = tp.replace_whitespace(text)\n",
    "    text = tp.remove_stopwords(text, list_of_stopwords)\n",
    "    return text\n",
    "\n",
    "\n",
    "def polarity_score_roberta(data):\n",
    "    # Do basic data pre-processing\n",
    "    data_preprocessing(data)\n",
    "\n",
    "    # Specify the maximum sequence length\n",
    "    max_length = 512  # Adjust this based on the model's maximum sequence length\n",
    "\n",
    "    # Tokenize and truncate/pad the input text\n",
    "    encoded_text = tokenizer(data, return_tensors='tf', max_length=max_length, truncation=True, padding=True)\n",
    "    \n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    scores_dict = {\n",
    "        \"roberta_neg\": scores[0],\n",
    "        'roberta_neu': scores[1],\n",
    "        'roberta_pos': scores[2]\n",
    "    }\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "# Define the function to generate labels\n",
    "def generate_roberta_labels(data, positive_threshold=0.5, negative_threshold=0.5):\n",
    "    # Get roberta scores\n",
    "    scores = polarity_score_roberta(data)\n",
    "    roberta_neg, roberta_neu, roberta_pos = scores['roberta_neg'], scores['roberta_neu'], scores['roberta_pos']\n",
    "\n",
    "    sentiment_results_dict = {'positive': roberta_pos, \n",
    "                              'negative': roberta_neg, \n",
    "                              'neutral': roberta_neu}\n",
    "\n",
    "    highest_sentiment = max(sentiment_results_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "    if highest_sentiment[0] == 'positive':    \n",
    "        if highest_sentiment[1] >= 0.8:        \n",
    "            return \"Strongly Positive\" \n",
    "        else:        \n",
    "            return \"Positive\"\n",
    "    elif highest_sentiment[0] == 'negative':    \n",
    "        if highest_sentiment[1] >= 0.8:        \n",
    "            return \"Strongly Negative\"\n",
    "        else:\n",
    "            return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "\n",
    "def process_row(df):\n",
    "    df['roberta_sentiment'] = df['review_detail'].apply(lambda x : generate_roberta_labels(x))\n",
    "    return df\n",
    "\n",
    "# NOTE: Change the number of threads depending on device's CPU core\n",
    "def parallel_processing(df, func, num_threads=3):\n",
    "    # Split the DataFrame into chunks for parallel processing\n",
    "    chunks = np.array_split(df, num_threads)\n",
    "\n",
    "    # Use ProcessPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        # Map the function to each chunk of the DataFrame in parallel\n",
    "        results = list(executor.map(func, chunks))\n",
    "\n",
    "    # Concatenate the results\n",
    "    result_df = pd.concat(results, ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            files.append(filepath)\n",
    "    return files\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    files_in_directory = list_files(split_dataset_directory_path)\n",
    "\n",
    "    # Create 'done' directory inside the 'split_dataset\n",
    "    if not os.path.exists(done_directory_path):\n",
    "        print(\"Creating 'done' directory...\")\n",
    "        os.makedirs(done_directory_path)\n",
    "\n",
    "    # Create 'sentiment' directory inside the 'split_dataset\n",
    "    if not os.path.exists(sentiment_directory_path):\n",
    "        print(\"Creating 'sentiment' directory...\")\n",
    "        os.makedirs(sentiment_directory_path)\n",
    "\n",
    "    for input_filepath in files_in_directory:\n",
    "\n",
    "        try:\n",
    "            base_filename = os.path.basename(input_filepath)\n",
    "            no_ext_base_filename = os.path.splitext(base_filename)[0]\n",
    "            output_filepath = sentiment_directory_path + no_ext_base_filename + \"_sentiment.csv\"\n",
    "\n",
    "            print(f\"Reading source the file: {input_filepath}\")\n",
    "            movie_reviews_df = pd.read_csv(input_filepath)\n",
    "            \n",
    "            print(\"Parallel processing of movie reviews...\")\n",
    "            movie_reviews_df = parallel_processing(movie_reviews_df, process_row)\n",
    "\n",
    "            # Save to csv file\n",
    "            movie_reviews_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "            print(f\"\\nSuccessfully saved the file with sentiments in {output_filepath}\")\n",
    "\n",
    "            # Move the file that was processed to the done folder\n",
    "            destination_file = os.path.join(done_directory_path, base_filename)\n",
    "            os.rename(input_filepath, destination_file)\n",
    "            \n",
    "            print(f\"\\nMove source file to done folder: {destination_file}\")\n",
    "        except Exception as err:\n",
    "            print(f\"ERROR: {err}\")\n",
    "            print(f\"File: {input_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
